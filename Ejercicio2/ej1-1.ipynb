{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorboard\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir 'logs/'\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import KFold\n",
    "# Se importan librerías para graficar.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Input, Dropout\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from keras.regularizers import l2, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_people_num = (df['Outcome'] == 0).sum()\n",
    "sick_people_num = (df['Outcome'] != 0).sum()\n",
    "total = df.shape[0]\n",
    "print(\"Healthy people: \" + str(healthy_people_num))\n",
    "print(\"Sick people: \" + str(sick_people_num))\n",
    "print(\"Total: \" + str(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "labels = ['No Diabéticos', 'Diabétos']\n",
    "sizes = [healthy_people_num,sick_people_num]\n",
    "colors = [\"green\",\"red\"]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes, labels=labels, explode= (0.01,0) , colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,)\n",
    "\n",
    "plt.title('Porcentaje de diabéticos.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df2[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(20, 10))\n",
    "plt.ylabel('Variables')\n",
    "plt.title(\"Boxplots\")\n",
    "ax = sns.boxplot(data = df2, \n",
    "  orient = 'h', \n",
    "  palette = 'Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=df2.columns, y=df2.isnull().sum())\n",
    "plt.xticks(rotation=45);\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(100*p.get_height()/df.shape[0], '.1f') + \"%\", \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 10), \n",
    "                   textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df2.corr()\n",
    "correlations['Outcome'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Insulin'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def separate_data_and_labels(df):\n",
    "        data = df.copy()\n",
    "        y_values = data[data.columns[-1]].values.reshape(data.shape[0], 1)\n",
    "        data = data.drop([data.columns[-1]], axis=1)\n",
    "        return data, y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train_val_df = df[msk]\n",
    "testData = df[~msk]\n",
    "\n",
    "train_val_data , y_train_val = separate_data_and_labels(train_val_df)\n",
    "x_test_data, y_test_values = separate_data_and_labels(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    return tn / (tn + fp + K.epsilon())\n",
    "\n",
    "\n",
    "def negative_predictive_value(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "    return tn / (tn + fn + K.epsilon())\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    sens_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return sens_keras\n",
    "\n",
    "def positive_predictive_value(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    ppv_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return ppv_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    # Plot the training loss.\n",
    "    plt.plot(history.history['loss'], 'r-')\n",
    "    # Plot the validation loss.\n",
    "    plt.plot(history.history['val_loss'], 'b-')\n",
    "    # X-axis label.\n",
    "    plt.xlabel('Epochs')\n",
    "    # Y-axis label.\n",
    "    plt.ylabel('Cost')\n",
    "    # Graph legend.\n",
    "    plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "    # Graph title.\n",
    "    plt.title('Loss Graph')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesPredictor:\n",
    "    def __init__(self, name):\n",
    "        \"\"\"DiabetesPredictor\n",
    "\n",
    "    This is a class contains the most part of the methods needed for the diabetes predictor,\n",
    "    first get the data of the csv file and then perform some methods to clean the data insid\n",
    "    and allows you to choose if it has to replace outliers or not and replace nulls values or not.\n",
    "\n",
    "    \"\"\"\n",
    "        self.name = name\n",
    "\n",
    "    def fit(self, train_df, y_train, val_df, y_val, replaceOutliers, replaceNulls, nullColumns, outliersColumnsMap, columnsToRemove, polyFeatDeg, binsDiscretizer, earlyStop, dropOut, regu, batchNormalization, learning_rate, momentum, decay, multilayer, layerUnits):\n",
    "        train_dataframe = train_df.copy()\n",
    "\n",
    "        self.columnsToRemove = columnsToRemove.copy()\n",
    "        self.nullCols = nullColumns.copy()\n",
    "        self.replaceNulls = replaceNulls\n",
    "        self.replaceOutliers = replaceOutliers\n",
    "        self.polyFeatDeg = polyFeatDeg\n",
    "        self.binsDiscretizer = binsDiscretizer\n",
    "        self.earlyStop = earlyStop\n",
    "        self.dropOut = dropOut\n",
    "        self.regu = regu\n",
    "        self.batchNormalization = batchNormalization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.multilayer = multilayer\n",
    "        self.layerUnits = layerUnits\n",
    "\n",
    "        self.replace_values_nulls = []\n",
    "        self.replace_values_outliers = []\n",
    "        self.outliersLimits = []\n",
    "        self.history = None\n",
    "\n",
    "        self.nullCols = [n for n in self.nullCols if n not in self.columnsToRemove]\n",
    "        self.outlierCols = {}\n",
    "        for k in outliersColumnsMap:\n",
    "            if k not in self.columnsToRemove:\n",
    "                self.outlierCols[k] = outliersColumnsMap[k].copy()\n",
    "\n",
    "        train_dataframe = self.__preprocess_data__(train_dataframe, training=True)\n",
    "\n",
    "        x_train_values = train_dataframe.values\n",
    "        y_train_values = y_train.copy()\n",
    "\n",
    "        self.input_shape = x_train_values.shape\n",
    "\n",
    "        x_val_df = val_df.copy()\n",
    "        x_val_df = self.__preprocess_data__(x_val_df)\n",
    "        x_val_values = x_val_df.values\n",
    "        y_val_values = y_val.copy()\n",
    "\n",
    "        self.model = self.__train_model__(x_train_values, y_train_values, x_val_values,y_val_values)\n",
    "\n",
    "        self.auc, self.spe, self.sen, self.ppv, self.npv, self.fpr, self.tpr = self.evaluate(x_val_df, y_val_values, testing = False)\n",
    "\n",
    "        self.aucTrain, self.speTrain, self.senTrain, self.ppvTrain, self.npvTrain, self.fprTrain, self.tprTrain = self.evaluate(train_dataframe, y_train_values, testing = False)\n",
    "        \n",
    "    def __preprocess_data__(self, data, training = False):\n",
    "        df = data.copy()\n",
    "        df = self.__remove_columns__(df)\n",
    "\n",
    "        if self.replaceNulls:\n",
    "            df[self.nullCols] = df[self.nullCols].replace(0,np.NaN)\n",
    "\n",
    "        if training:\n",
    "            self.replace_values_outliers = self.__get_cols_median__(df)\n",
    "            self.outliersLimits = self.__get_outliers_limits__(df)\n",
    "\n",
    "        if(self.replaceOutliers):\n",
    "            df = self.__replace_outliers__(df)\n",
    "\n",
    "        if training:\n",
    "            self.replace_values_nulls = self.__get_cols_median__(df)\n",
    "\n",
    "        if(self.replaceNulls):\n",
    "            df = self.__replace_nulls__(df)\n",
    "\n",
    "        if (self.polyFeatDeg > 0):\n",
    "            poly = PolynomialFeatures(degree=self.polyFeatDeg)\n",
    "            polyArray = poly.fit_transform(df)\n",
    "            c = poly.get_feature_names(df.columns)\n",
    "            df = pd.DataFrame(polyArray, columns = c)\n",
    "\n",
    "        if (self.binsDiscretizer > 1):\n",
    "            disc = KBinsDiscretizer(n_bins=self.binsDiscretizer, encode='ordinal', strategy='uniform')\n",
    "            df = disc.fit_transform(df)\n",
    "\n",
    "        if training:\n",
    "            self.mean_cols = self.__get_cols_mean__(df)\n",
    "            self.std_cols = self.__get_cols_std__(df)\n",
    "\n",
    "        df = self.__normalize_data__(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __get_cols_mean__(self, data):\n",
    "        meandf = data.mean(axis = 0)\n",
    "        meandf.columns = data.columns\n",
    "        return meandf\n",
    "\n",
    "    def __get_cols_std__(self, data):\n",
    "        stddf= data.std(axis = 0)\n",
    "        stddf.columns = data.columns\n",
    "        return stddf\n",
    "\n",
    "    def __get_cols_median__(self, data):\n",
    "        mediandf = data.median(axis = 0)\n",
    "        mediandf.columns = data.columns\n",
    "        return mediandf\n",
    "\n",
    "    def __get_outliers_limits__(self, data):\n",
    "        df = pd.DataFrame(np.zeros((1,len(data.columns))), columns=data.columns)\n",
    "        df = df.astype('object')\n",
    "        for col in data.columns:\n",
    "            col_min = 0\n",
    "            col_max = np.Infinity\n",
    "            if (col in self.outlierCols):\n",
    "                #Cálculo de parámetros para obtener los outliers.\n",
    "                q1 = data[col].quantile(0.25)\n",
    "                q3 = data[col].quantile(0.75)\n",
    "                iqr = q3-q1\n",
    "                #Límites de los valores tipicos.\n",
    "                lower_tail = q1 - 1.5 * iqr\n",
    "                upper_tail = q3 + 1.5 * iqr\n",
    "\n",
    "                col_min = min(lower_tail, self.outlierCols[col][0])\n",
    "                col_max = max(upper_tail,self.outlierCols[col][1])\n",
    "            lims = [col_min, col_max]\n",
    "            df[col][0] = lims\n",
    "        return df\n",
    "\n",
    "    def __replace_outliers__(self, dataframe):\n",
    "        #Para cada columna (excepto para el Outcome) se reemplazan los outliers por su mediana.\n",
    "        df = dataframe.copy()\n",
    "        for col in df.columns:\n",
    "            out_min = self.outliersLimits[col][0][0]\n",
    "            out_max = self.outliersLimits[col][0][1]\n",
    "            for i in df[col]:\n",
    "                if i > out_max or i < out_min:\n",
    "                    df[col] = df[col].replace(i, self.replace_values_outliers[col])\n",
    "        return df\n",
    "\n",
    "    def __replace_nulls__(self, dataframe):\n",
    "        df = dataframe.copy()\n",
    "        for col in self.nullCols:\n",
    "            df[col]=df[col].replace(np.NaN, self.replace_values_nulls[col])\n",
    "        return df\n",
    "\n",
    "    def __normalize_data__(self, dataframe):\n",
    "        # Mean, columnar axis.\n",
    "        df = dataframe.copy()\n",
    "        for col in df.columns:\n",
    "            if (self.std_cols[col] != 0):\n",
    "                df[col] = (df[col] - self.mean_cols[col]) / self.std_cols[col]\n",
    "            else:\n",
    "                df[col] = 0\n",
    "        return df\n",
    "\n",
    "    def __remove_columns__(self, dataframe):\n",
    "        df = dataframe.copy()\n",
    "        df = df.drop(self.columnsToRemove, axis=1)\n",
    "        return df\n",
    "\n",
    "    def __model_builder__(self):\n",
    "        model = Sequential()\n",
    "        initializer = tf.keras.initializers.GlorotNormal(seed=7)\n",
    "        if (self.multilayer):\n",
    "            model.add(Dense(self.layerUnits[0], kernel_initializer=initializer, bias_initializer=initializer, input_shape=(self.input_shape[1],), activation='relu'))\n",
    "            model.add(Dense(self.layerUnits[1], kernel_initializer=initializer, bias_initializer=initializer, activation='relu'))\n",
    "            model.add(Dense(1, kernel_initializer=initializer, bias_initializer=initializer,activation='sigmoid'))\n",
    "        else:\n",
    "            if(self.dropOut):\n",
    "                model.add(Dropout(0.5,input_shape=(self.input_shape[1],)))\n",
    "            elif(self.batchNormalization):\n",
    "                model.add(BatchNormalization(input_shape=(self.input_shape[1],)))\n",
    "            \n",
    "            if(self.regu == 'l1' or self.regu == 'l2'):\n",
    "                reg = reg_wrapper(self.regu, 0.1)\n",
    "                model.add(Dense(1, kernel_initializer=initializer,bias_initializer=initializer,kernel_regularizer=reg, activation='sigmoid'))\n",
    "            else:\n",
    "                model.add(Dense(1, kernel_initializer=initializer, bias_initializer=initializer,activation='sigmoid'))\n",
    "\n",
    "        learning_rate = 1e-3\n",
    "        momentum = 0.99\n",
    "        decay = 1e-4\n",
    "\n",
    "        model.compile(optimizer=optimizers.SGD(learning_rate=learning_rate, momentum=momentum, decay = decay),\n",
    "                        loss=keras.losses.BinaryCrossentropy(),\n",
    "                        metrics=[tf.keras.metrics.AUC()])\n",
    "        return model\n",
    "\n",
    "    def __train_model__(self, x_train, y_train, x_val, y_val):        \n",
    "        stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "        model = self.__model_builder__()\n",
    "\n",
    "        ckpt_model = 'saved_models/'+ self.name\n",
    "        checkpoint = ModelCheckpoint(ckpt_model, \n",
    "                            monitor='val_loss',\n",
    "                            verbose=0,\n",
    "                            save_best_only=True,\n",
    "                            mode='min')\n",
    "\n",
    "        cbks = [checkpoint]\n",
    "        if (self.earlyStop):\n",
    "            cbks.append(stop_early)\n",
    "        \n",
    "        history = model.fit(x_train, y_train, epochs=125, validation_data = (x_val, y_val), verbose=0, callbacks = cbks)\n",
    "\n",
    "        self.history = history\n",
    "        # Plot Loss\n",
    "        plot_loss(history)        \n",
    "\n",
    "        # Return trained model\n",
    "        return model\n",
    "\n",
    "    def evaluate(self, x_val_df, y_val, testing=True):\n",
    "        x_df = x_val_df.copy()\n",
    "        predictions, rounded_preds = self.predict(x_df, testing=testing)\n",
    "\n",
    "        fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_val, predictions)\n",
    "        auc_keras = auc(fpr_keras, tpr_keras)\n",
    "        spe = specificity(y_val.astype(float),  rounded_preds.astype(float))\n",
    "        sen = sensitivity(y_val.astype(float),  rounded_preds.astype(float))\n",
    "        ppv = positive_predictive_value(y_val.astype(float), rounded_preds.astype(float))\n",
    "        npv = negative_predictive_value(y_val.astype(float), rounded_preds.astype(float))\n",
    "\n",
    "        if (testing):\n",
    "            self.plot_roc(fpr_keras, tpr_keras, auc_keras)\n",
    "            print('AUC ' + str(auc_keras))\n",
    "            print('Specificity: ' + str(spe))\n",
    "            print('Sensitivity: ' + str(sen))\n",
    "            print('Positive Predictive Value: ' + str(ppv))\n",
    "            print('Negative Predictive Value: ' + str(npv))\n",
    "\n",
    "        return auc_keras, spe, sen, ppv, npv, fpr_keras, tpr_keras\n",
    "\n",
    "    def predict(self, x_val_df, testing=True):\n",
    "        x_df = x_val_df.copy()\n",
    "        if (testing):\n",
    "            x_df = self.__preprocess_data__(x_df)\n",
    "        predictions = self.model(x_df.values)\n",
    "        rounded_preds = np.rint(predictions)\n",
    "        return predictions, rounded_preds\n",
    "\n",
    "    def plot_roc_validation(self):\n",
    "        self.plot_roc(self.fpr, self.tpr, self.auc)\n",
    "\n",
    "    def plot_roc(self, fpr, tpr, auc):\n",
    "        plt.figure(1)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.plot(fpr, tpr, label=' (Area = {:.3f})'.format(auc))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.title('ROC curve ' + self.name)\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_wrapper(type, value):\n",
    "    if type == 'l2':\n",
    "        return regularizers.l2(value)\n",
    "    if type == 'l1':\n",
    "        return regularizers.l1(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "def cross_val(name, replaceOutliers=False, replaceNulls=False, nullColumns=[], outliersColumnsMap={}, columnsToRemove=[], polyFeatDeg = -1, binsDiscretizer = -1, earlyStop = False, dropOut = False, regu = '', batchNormalization = False, learning_rate = 1e-5, momentum= 0.99, decay = 1e-4, multilayer = False, layerUnits = []):\n",
    "    folds = kf.split(train_val_data)\n",
    "    bestModel = None\n",
    "    aucs = []\n",
    "    spes = []\n",
    "    sens = []\n",
    "    ppvs = []\n",
    "    npvs = []\n",
    "    aucsTrain = []\n",
    "    i = 0\n",
    "    for train_index, val_index in folds:\n",
    "        print('Fold ' + str(i))\n",
    "        newModel = DiabetesPredictor(name + str(i))\n",
    "        i += 1\n",
    "        X_train, X_val = train_val_data.iloc[train_index], train_val_data.iloc[val_index]\n",
    "        Y_train, Y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "        newModel.fit(X_train, Y_train, X_val, Y_val, replaceOutliers=replaceOutliers, replaceNulls=replaceNulls, nullColumns = nullColumns, outliersColumnsMap=outliersColumnsMap, columnsToRemove=columnsToRemove, polyFeatDeg=polyFeatDeg, binsDiscretizer=binsDiscretizer, earlyStop=earlyStop, dropOut=dropOut, regu=regu, batchNormalization=batchNormalization, learning_rate = learning_rate, momentum= momentum, decay = decay, multilayer = multilayer, layerUnits = layerUnits)\n",
    "        aucs.append(newModel.auc)\n",
    "        spes.append(newModel.spe)\n",
    "        sens.append(newModel.sen)\n",
    "        ppvs.append(newModel.ppv)\n",
    "        npvs.append(newModel.npv)\n",
    "        aucsTrain.append(newModel.aucTrain)\n",
    "        if (bestModel==None or newModel.auc > bestModel.auc):\n",
    "            bestModel = newModel\n",
    "    \n",
    "    aucTrainMean = np.mean(np.array(aucsTrain))\n",
    "    aucMean = np.mean(np.array(aucs))\n",
    "    speMean = np.mean(np.array(spes))\n",
    "    senMean = np.mean(np.array(sens))\n",
    "    ppvMean = np.mean(np.array(ppvs))\n",
    "    npvMean = np.mean(np.array(npvs))\n",
    "\n",
    "    print('AUC Val: ' + str(aucMean))\n",
    "    print('AUC Train: ' + str(aucTrainMean))\n",
    "    print('Specificity: ' + str(speMean))\n",
    "    print('Sensitivity: ' + str(senMean))\n",
    "    print('Positive Predictive Value: ' + str(ppvMean))\n",
    "    print('Negative Predictive Value: ' + str(npvMean))\n",
    "\n",
    "    \n",
    "    return bestModel, aucMean, speMean, senMean, ppvMean, npvMean, aucTrainMean"
   ]
  },
  {
   "source": [
    "### Sin limpiar datos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplePred, aucPred, spePred, senPred, ppvPred, npvPred, aucTrainPred = cross_val('predictor')"
   ]
  },
  {
   "source": [
    "## Limpiando Datos\n",
    "#### Reemplazando Nulls por la mediana"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repNullPred, aucrepNull, sperepNull, senrepNull, ppvrepNull, npvrepNull, aucTrainRepNull = cross_val('rep_nulls_predictor',replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'])"
   ]
  },
  {
   "source": [
    "#### Reemplazando Outliers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val('rep_outliers_predictor', replaceOutliers=True, outliersColumnsMap={'BMI': [18.5, 50],'BloodPressure':[40,120],'SkinThickness': [0, np.Infinity], 'Pregnancies': [0, np.Infinity]})"
   ]
  },
  {
   "source": [
    "#### Reemplazando Outliers y nulls"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val('rep_outliers_nulls_predictor',replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], replaceOutliers=True, outliersColumnsMap={'BMI': [18.5, 50],'BloodPressure':[40,120],'SkinThickness': [0, np.Infinity], 'Pregnancies': [0, np.Infinity]})"
   ]
  },
  {
   "source": [
    "Comparando los resultados anteriores:\n",
    "- AUC reemplazando nulls: 0.849\n",
    "- AUC reemplazando outliers y nulls: 0.847\n",
    "- AUC reemplazando outliers: 0.840\n",
    "\n",
    "Dado que el valor más alto es reemplazando nulls, continuamos el análisis con este parámetro."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Removiendo Columnas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestAuc = aucrepNull\n",
    "tryRemovingCols = ['BloodPressure', 'Age', 'DiabetesPedigreeFunction', 'Pregnancies', 'SkinThickness']\n",
    "removeCols = []\n",
    "removeColsAuc = []\n",
    "nullCols = ['Glucose','BloodPressure','SkinThickness','BMI']\t\n",
    "\t\t\n",
    "for col in tryRemovingCols:\n",
    "\tprint(col)\t\n",
    "\tremColPred, aucremCol, speremCol, senremCol, ppvremCol, npvremCol, aucTrainRemCol = cross_val('rem_col_predictor_' + col ,replaceNulls=True, nullColumns=nullCols, columnsToRemove=[col])\n",
    "\tif (aucremCol > bestAuc):\n",
    "\t\tremoveCols.append(col)\n",
    "\t\tremoveColsAuc.append(aucremCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(removeCols)\n",
    "print(removeColsAuc)"
   ]
  },
  {
   "source": [
    "#### Eliminando todas las columnas que superaron el valor anterior."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val('rem_cols_predictor' ,replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=removeCols)"
   ]
  },
  {
   "source": [
    "AUC:\n",
    "- Eliminando Age: 0.85\n",
    "- Eliminando 'Age', 'SkinThickness': 0.849\n",
    "- Eliminando SkinThickness: 0.849\n",
    "\n",
    "Procedemos eliminando la columna de Age"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucPolys = []\n",
    "for i in range(1,5):\n",
    "    print(i)\n",
    "    polyPred, aucPoly, spePoly, senPoly, ppvPoly, npvPoly, aucTrainPoly = cross_val('featPol' + str(i) + '_' ,replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'], polyFeatDeg=i)\n",
    "    aucPolys.append(aucPoly)\n",
    "\n",
    "bestPolDeg = aucPolys.index(max(aucPolys)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aucPolys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Degree: ' + str(bestPolDeg) + \" --- AUC: \" + str(max(aucPolys)))"
   ]
  },
  {
   "source": [
    "#### Vuelvo a probar con EarlyStopping (había overfitting)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucEarlyPolys = []\n",
    "for i in range(1,5):\n",
    "    print(i)\n",
    "    polyPred, aucPoly, spePoly, senPoly, ppvPoly, npvPoly, aucTrainPoly = cross_val('featPolEarly' + str(i) + '_' ,replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'], polyFeatDeg=i, earlyStop=True)\n",
    "    aucEarlyPolys.append(aucPoly)\n",
    "\n",
    "bestPolDeg = aucEarlyPolys.index(max(aucEarlyPolys)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Degree: ' + str(bestPolDeg) + \" --- AUC: \" + str(max(aucPolys)))"
   ]
  },
  {
   "source": [
    "#### Con reg L1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucReguPolys = []\n",
    "for i in range(1,6):\n",
    "    print(i)\n",
    "    polyPred, aucPoly, spePoly, senPoly, ppvPoly, npvPoly, aucTrainPoly = cross_val('featPolEarly' + str(i) + '_' ,replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'], polyFeatDeg=i, regu='l1')\n",
    "    aucReguPolys.append(aucPoly)\n",
    "\n",
    "bestPolReguL1Deg = aucReguPolys.index(max(aucReguPolys)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Degree: ' + str(bestPolReguL1Deg) + \" --- AUC: \" + str(max(aucPolys)))"
   ]
  },
  {
   "source": [
    "#### Con reg L2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucReguPolys = []\n",
    "for i in range(1,6):\n",
    "    print(i)\n",
    "    polyPred, aucPoly, spePoly, senPoly, ppvPoly, npvPoly, aucTrainPoly = cross_val('featPolEarly' + str(i) + '_' ,replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'], polyFeatDeg=i, regu='l2')\n",
    "    aucReguPolys.append(aucPoly)\n",
    "\n",
    "bestPolReguL2Deg = aucReguPolys.index(max(aucReguPolys)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Degree: ' + str(bestPolReguL2Deg) + \" --- AUC: \" + str(max(aucPolys)))"
   ]
  },
  {
   "source": [
    "#### Con DropOut"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val('dropOut' ,replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'], polyFeatDeg=4, dropOut=True)"
   ]
  },
  {
   "source": [
    "#### Con Batch Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val(\"batch\",replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'])"
   ]
  },
  {
   "source": [
    "Entre EarlyStopping, DropOut y regularizadores, los mejores resultados se obtuvieron con regularizadores.\n",
    "\n",
    "Aunque no se obtuvieron mejoras utilizando featuresPolinomiales y regularización, estos se utilizarán en los próximos análisis a fin de evitar overfitting."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val('l2' ,replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'],polyFeatDeg=3, regu=\"l2\")"
   ]
  },
  {
   "source": [
    "### Variación del learning rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucLrPolys = []\n",
    "lrs = [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1]\n",
    "for lr in lrs:\n",
    "    print(lr)\n",
    "    lrPred, aucLr, speLr, senLr, ppvLr, npvLr, aucTrainLr = cross_val('lr'+str(lr) ,replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'],polyFeatDeg=3, regu=\"l2\", learning_rate=lr)\n",
    "    aucLrPolys.append(aucLr)\n",
    "\n",
    "bestLr = lrs[aucLrPolys.index(max(aucLrPolys))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best LR: ' + str(bestLr) + \" --- AUC: \" + str(max(aucLrPolys)))"
   ]
  },
  {
   "source": [
    "### Variando momentum"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucsMom = []\n",
    "moms = [0.9, 0.99, 0.999, 0.9999]\n",
    "for mom in moms:\n",
    "    print(mom)\n",
    "    momPred, aucMom, speMom, senMom, ppvMom, npvMom, aucTrainMom = cross_val('lr'+str(lr) ,replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'], polyFeatDeg=3, regu=\"l2\", learning_rate=1e-5, momentum=mom)\n",
    "    aucsMom.append(aucMom)\n",
    "\n",
    "bestMom = moms[aucsMom.index(max(aucsMom))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Mom: ' + str(bestMom) + \" --- AUC: \" + str(max(aucsMom)))"
   ]
  },
  {
   "source": [
    "### Variando learning rate decay"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucsDecays = []\n",
    "decays = [1e-3, 1e-4, 1e-5]\n",
    "for dec in decays:\n",
    "    print(dec)\n",
    "    decPred, aucDec, speDec, senDec, ppvDec, npvDec, aucTrainDec = cross_val(\"dec\"+str(dec),replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'], polyFeatDeg=3, regu=\"l2\", learning_rate=1e-5, momentum=0.999, decay=dec)\n",
    "    aucsDecays.append(aucDec)\n",
    "\n",
    "bestDec = decays[aucsDecays.index(max(aucsDecays))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Decay: ' + str(bestDec) + \" --- AUC: \" + str(max(aucsDecays)))"
   ]
  },
  {
   "source": [
    "### Multilayer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val('multi',replaceNulls=True, nullColumns=['Glucose','BloodPressure','SkinThickness','BMI'], columnsToRemove=['Age'], polyFeatDeg=3, learning_rate=1, momentum=0.9, decay=1e-5, multilayer=True, earlyStop = True)"
   ]
  }
 ]
}