{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd035d319f83ab08ecd497627408ea36f4ad0a0ef0ec31f33b5bf93fec9ccad80dc",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "35d319f83ab08ecd497627408ea36f4ad0a0ef0ec31f33b5bf93fec9ccad80dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Ejercicio 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "source": [
    "## Importación de los sets de train y test.\n",
    "Se eliminan los headers y los footers.\n",
    "Se verifican los tamaños de los sets y las categorías a las que corresponden los artículos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length train: 11314\nLength test: 7532\n['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, remove=('headers', 'footers'))\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, remove=('headers', 'footers'))\n",
    "print('Length train: '+ str(len(twenty_train.data)))\n",
    "print('Length test: '+ str(len(twenty_test.data)))\n",
    "\n",
    "print(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NEW DATA\nI was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\nNEW DATA\nA fair number of brave souls who upgraded their SI clock oscillator have\nshared their experiences for this poll. Please send a brief message detailing\nyour experiences with the procedure. Top speed attained, CPU rated speed,\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\nfunctionality with 800 and 1.4 m floppies are especially requested.\n\nI will be summarizing in the next two days, so please add to the network\nknowledge base if you have done the clock upgrade and haven't answered this\npoll. Thanks.\nNEW DATA\nwell folks, my mac plus finally gave up the ghost this weekend after\nstarting life as a 512k way back in 1985.  sooo, i'm in the market for a\nnew machine a bit sooner than i intended to be...\n\ni'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\nof questions that (hopefully) somebody can answer:\n\n* does anybody know any dirt on when the next round of powerbook\nintroductions are expected?  i'd heard the 185c was supposed to make an\nappearence \"this summer\" but haven't heard anymore on it - and since i\ndon't have access to macleak, i was wondering if anybody out there had\nmore info...\n\n* has anybody heard rumors about price drops to the powerbook line like the\nones the duo's just went through recently?\n\n* what's the impression of the display on the 180?  i could probably swing\na 180 if i got the 80Mb disk rather than the 120, but i don't really have\na feel for how much \"better\" the display is (yea, it looks great in the\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\ntaking the disk size and money hit to get the active display?  (i realize\nthis is a real subjective question, but i've only played around with the\nmachines in a computer store breifly and figured the opinions of somebody\nwho actually uses the machine daily might prove helpful).\n\n* how well does hellcats perform?  ;)\n\nthanks a bunch in advance for any info - if you could email, i'll post a\nsummary (news reading time is at a premium with finals just around the\ncorner... :( )\n--\nTom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\nNEW DATA\nRobert J.C. Kyanko (rob@rjck.UUCP) wrote:\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\n> > Anyone know about the Weitek P9000 graphics chip?\n> As far as the low-level stuff goes, it looks pretty nice.  It's got this\n> quadrilateral fill command that requires just the four points.\n\nDo you have Weitek's address/phone number?  I'd like to get some information\nabout this chip.\n\nNEW DATA\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\n>>>\"Clear caution & warning memory.  Verify no unexpected\n>>>errors. ...\".  I am wondering what an \"expected error\" might\n>>>be.  Sorry if this is a really dumb question, but\n> \n> Parity errors in memory or previously known conditions that were waivered.\n>    \"Yes that is an error, but we already knew about it\"\n> I'd be curious as to what the real meaning of the quote is.\n> \n> tom\n\n\nMy understanding is that the 'expected errors' are basically\nknown bugs in the warning system software - things are checked\nthat don't have the right values in yet because they aren't\nset till after launch, and suchlike. Rather than fix the code\nand possibly introduce new bugs, they just tell the crew\n'ok, if you see a warning no. 213 before liftoff, ignore it'.\n"
     ]
    }
   ],
   "source": [
    "# A fin de analizar el formato de los artículos, se imprimen los primeros 5.\n",
    "for i in range(5):\n",
    "    print('NEW DATA')\n",
    "    print(twenty_train.data[i])"
   ]
  },
  {
   "source": [
    "## Preprocesamiento.\n",
    "\n",
    "Antes de alimentar al modelo con los textos, es necesario transformarlos a fin de aumentar la consistencia de la métrica a obtener. Para ello se siguen una serie de pasos que se explican a continuación,.\n",
    "\n",
    "### Tokenización\n",
    "La tokenización es el proceso por el cual una gran cantidad de texto se divide en partes más pequeñas llamadas tokens. Los tokens serán utilizados para posteriormente procesar individualmente cada palabra del texto.\n",
    "### Lematización\n",
    "La lematización tiene en cuenta el análisis morfológico de las palabras para transformarlas. Distintas palabras que corresponden a un mismo concepto pero varían levemente -o no varían- en su significado son llevadas a una forma en común que las representa a todas. En general esta forma coincide con el formato de las palabras que se encuentran en el diccionario. Por ejemplo, los verbos son transformados en sus infinitivos y los plurales en singulares.\n",
    "### Stopwords\n",
    "Las \"stopwords\" son las palabras comunes más comunes en un idioma. Por esta razón, tenerlas en cuenta para el modelo que se desea implementar no aporta información relevante en cuanto a la categorización de los textos e implica mayor tiempo de procesamiento. Como ejemplo las palabras \"a\", \"de\", \"y\", \"el\" y \"o\" son consideradas stopwords en el idioma español.\n",
    "### Estemizado\n",
    "El estemizado reduce las palabras a su raíz y es utilizado en el preprocesamiento del texto con el objetivo de unificar términos cuyos sufijos varían pero pertenecen a las mismas clases. Por ejemplo, las palabras \"niños\" y \"niñez\" podrían ser reducidas mediante esta técnica a la raíz \"niñ\".\n",
    "### Filtrado de no alfabéticos.\n",
    "Una vez aplicadas las técnicas mencionadas anteriormente, es probable que el vocabulario siga contando con tokens que no sean palabras. Estos tokens, como signos de puntuación o números, no aportan información útil al procesamiento por lo tanto deben ser eliminados para mejorar el rendimiento del modelo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### NLTK\n",
    "La librería NLTK (Natural Language Toolkit) contiene herramientas y paquetes con funcionalidades relacionadas al procesamiento estadístico del lenguaje natural. Entre estas herramientas se encuentran las explicadas anteriormente (lematización, estemizado, etc.).\n",
    "\n",
    "#### Selección del Stemmer\n",
    "Los tres principales algoritmos de stemming que se utilizan son: Porter Stemmer, Snowball Stemmer y Lancaster Stemmer.  \n",
    "El algoritmo de Porter es el más antiguo de los tres algoritmos mencionados y es el menos \"agresivo\". A su vez, implica un alto costo en términos de tiempo de procesamiento. Por su parte, el algoritmo Snowball presenta algunas mejoras con respecto al de Porter y en cuanto a tiempo de procesamiento es algo mas rápido. Por último, el algoritmo de Lancaster es el más rapido de los tres pero el resultado del lematizado es poco intuitivo y de difícil análisis.  \n",
    "Analizando los beneficios y contras de cada lematizador, decidimos utilizar el ***Snowball Stemmer***, provisto por NLTK. \n",
    "\n",
    "#### Selección del Lematizador\n",
    "***Wordnet*** es una base de datos léxica que proporciona relaciones semánticas entre sus palabras. Wordnet agrupa distintos elementos de datos que son semánticamente equivalentes en \"synsets\". Es una de las primeras y más utilizadas técnicas de lematización y se encuentra disponible con NLTK. Por estas razones, decidimos utilizar dicho lematizador.  \n",
    "Sin embargo, este lematizador trata por defecto a todas las palabras como si fueran sustantivos. Así, muchos verbos se mantienen iguales luego de la lematización. Para solucionar este problema, es necesario utilizar etiquetas \"***POS***\" (Part Of Speech). Por cada palabra, se llama al lematizador con una etiqueta que define su tipo (adjetivo, sustantivo, verbo, etc.). \n",
    "\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\pabli\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\pabli\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\pabli\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\pabli\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger') #For POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización del lematizador y del stemmer.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traduce etiquetas POS de la función nltk.pos_tag en etiquetas que acepta WordNetLemmatizer.\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(data_in):\n",
    "    filtered_arts = list()\n",
    "    aux_porc = -1\n",
    "    for i in range(len(data_in)):\n",
    "        ########## PRINT PROGRESS ############\n",
    "        porc = int((i/len(data_in))*100)\n",
    "        if (porc!=aux_porc):\n",
    "            print(str(\"\\rProcesado: \" + str(porc) + \"%\"), end=\"\")\n",
    "            porc = aux_porc\n",
    "        ############## TOKENIZE ################\n",
    "        tokenized = word_tokenize(data_in[i])\n",
    "        ############## LEMMATIZE ################\n",
    "        pos_tagged = nltk.pos_tag(tokenized)\n",
    "        wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "        lemmatized = list()\n",
    "        for word, tag in wordnet_tagged:\n",
    "            if tag is None:\n",
    "                lemmatized.append(word)\n",
    "            else:        \n",
    "                lemmatized.append(lemmatizer.lemmatize(word, tag))\n",
    "        ############## STOP WORDS ################\n",
    "        stop = [lem for lem in lemmatized if lem not in stopwords.words('english')]\n",
    "        ############## STEMMER ################\n",
    "        stemmed=[stemmer.stem(w) for w in stop]\n",
    "        ############## REMOVE NON ALPHA ################\n",
    "        alpha = [st for st in stemmed if st.isalpha()]\n",
    "        ############## JOIN ################\n",
    "        filtered_arts.append(\" \".join(alpha))\n",
    "    return filtered_arts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Procesado: 99%"
     ]
    }
   ],
   "source": [
    "filteredArts = pre_process_data(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se guarda el resultado del preprocesamiento en un archivo mediante la librería Pickle.\n",
    "import pickle\n",
    "\n",
    "with open('art_filt.pkl', 'wb') as fp:\n",
    "    pickle.dump(filteredArts, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se lee el resultado del preprocesamiento guardado en un archivo mediante la librería Pickle.\n",
    "#import pickle\n",
    "with open('art_filt.pkl','rb') as fp:\n",
    "    articlesList = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11314\ni wonder anyon could enlighten car i saw day it sport car look late earli it call bricklin the door realli small in addit front bumper separ rest bodi this i know if anyon tellm model name engin spec year product car make histori whatev info funki look car pleas\n"
     ]
    }
   ],
   "source": [
    "#Se verifica que los artículos hayan sido obtenidos correctamente.\n",
    "print(str(len(articlesList)))\n",
    "print(articlesList[0])"
   ]
  },
  {
   "source": [
    "### Vectorización\n",
    "Explicación de Vectorización y de Count Vectorizer\n",
    "Explicación de min_df y max_df"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11314, 12594)"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "count_vectorizer = CountVectorizer(max_df = 0.1, min_df = 5)\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, min_df = 10)\n",
    "trainedData = count_vectorizer.fit_transform(articlesList)\n",
    "trainedDataArray = trainedData.toarray() #sparsed -> expanded matrix\n",
    "trainedData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se imprime el vocabulario resultante en un txt con el fin de analizarlo visualmente.\n",
    "with open('word_list.txt', 'w') as fp:\n",
    "    for word in count_vectorizer.get_feature_names():\n",
    "        fp.write(word + \"\\n\")"
   ]
  },
  {
   "source": [
    "### Pandas\n",
    "Explicación de Pandas y su uso"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   aa  aaa  aamir  aaron  ab  abandon  abbey  abbott  abbrevi  abc  ...  zoom  \\\n",
       "0   0    0      0      0   0        0      0       0        0    0  ...     0   \n",
       "1   0    0      0      0   0        0      0       0        0    0  ...     0   \n",
       "2   0    0      0      0   0        0      0       0        0    0  ...     0   \n",
       "3   0    0      0      0   0        0      0       0        0    0  ...     0   \n",
       "4   0    0      0      0   0        0      0       0        0    0  ...     0   \n",
       "\n",
       "   zr  zs  zterm  zu  zubov  zv  zy  zz  targetCode  \n",
       "0   0   0      0   0      0   0   0   0           7  \n",
       "1   0   0      0   0      0   0   0   0           4  \n",
       "2   0   0      0   0      0   0   0   0           4  \n",
       "3   0   0      0   0      0   0   0   0           1  \n",
       "4   0   0      0   0      0   0   0   0          14  \n",
       "\n",
       "[5 rows x 12595 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>aaa</th>\n      <th>aamir</th>\n      <th>aaron</th>\n      <th>ab</th>\n      <th>abandon</th>\n      <th>abbey</th>\n      <th>abbott</th>\n      <th>abbrevi</th>\n      <th>abc</th>\n      <th>...</th>\n      <th>zoom</th>\n      <th>zr</th>\n      <th>zs</th>\n      <th>zterm</th>\n      <th>zu</th>\n      <th>zubov</th>\n      <th>zv</th>\n      <th>zy</th>\n      <th>zz</th>\n      <th>targetCode</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 12595 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "import pandas as pd\n",
    "alpha = 1\n",
    "cols = count_vectorizer.get_feature_names()\n",
    "df = pd.DataFrame(trainedDataArray, columns = cols)\n",
    "df[\"targetCode\"] = twenty_train.target\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Entrenamiento del modelo.\n",
    "### Naive Bayes\n",
    "Explicación y fórmulas\n",
    "#### Probabilidades a priori y a posteriori.\n",
    "#### Likelihoods y LogLikelihoods\n",
    "### Smoothing Laplaciano"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cat = 20\n",
    "N_arts = df.values.shape[0]\n",
    "N_words = df.values.shape[1] - 1 #-1 target's columns\n",
    "prioriProbs = list()\n",
    "catProbs = list()\n",
    "\n",
    "for i in range(N_cat):\n",
    "    catOcurrency = sum(df.loc[df[\"targetCode\"] == i].drop(\"targetCode\", axis=1).values) + alpha #Sum of category ocurrencies\n",
    "    prioriProbs.append(np.log(catOcurrency/sum(catOcurrency)))\n",
    "    catProbs.append(np.log(df.loc[df[\"targetCode\"] == i].shape[0]/N_arts))\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Testeo del modelo.\n",
    "### Métricas\n",
    "Explicación de métricas y de cual utilizamos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Prueba con set de Train."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy Train: 0.8607035531200283\n"
     ]
    }
   ],
   "source": [
    "predictionOk = 0\n",
    "for n_art in range(N_arts):\n",
    "    maxIdx = -1\n",
    "    maxLogL = -float('inf')\n",
    "    for i in range(N_cat):\n",
    "        logL = np.dot((trainedDataArray[n_art]),prioriProbs[i])+catProbs[i]\n",
    "        if (logL > maxLogL):\n",
    "            maxLogL = logL\n",
    "            maxIdx = i\n",
    "    if(maxIdx == twenty_train.target[n_art]):\n",
    "        predictionOk += 1\n",
    "print(\"Accuracy Train: \" + str(predictionOk/N_arts))"
   ]
  },
  {
   "source": [
    "### Prueba con set de Test."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Procesado: 99%"
     ]
    }
   ],
   "source": [
    "#Preprocesamiento del set de Test.\n",
    "filteredTestArts = pre_process_data(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se guarda el resultado del preprocesamiento en un archivo mediante la librería Pickle.\n",
    "with open('art_filt_test.pkl', 'wb') as fp:\n",
    "    pickle.dump(filteredTestArts, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7532\ni littl confus model bonnevill i hear le se lse sse ssei could someon tell differ far featur perform i also curious know book valu prefer model and much less book valu usual get in word much demand time year i hear earli summer best time buy\n"
     ]
    }
   ],
   "source": [
    "#Se lee el resultado del preprocesamiento guardado en un archivo mediante la librería Pickle.\n",
    "with open('art_filt_test.pkl','rb') as fp:\n",
    "    testArticlesList = pickle.load(fp)\n",
    "#Se verifica que los artículos hayan sido obtenidos correctamente.\n",
    "print(len(testArticlesList))\n",
    "print(testArticlesList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7532, 12594)\n"
     ]
    }
   ],
   "source": [
    "# Vectorización del set de test.\n",
    "testData = count_vectorizer.transform(testArticlesList)\n",
    "testDataArray = testData.toarray()\n",
    "print(testData.shape)"
   ]
  },
  {
   "source": [
    "### Obtención del accuracy del modelo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy Test: 0.7308815719596389\n"
     ]
    }
   ],
   "source": [
    "predictionOk = 0\n",
    "for n_art in range(len(twenty_test.target)):\n",
    "    maxIdx = -1\n",
    "    maxLogL = -float('inf')\n",
    "    for i in range(N_cat):\n",
    "        logL = np.dot((testDataArray[n_art]),prioriProbs[i])+catProbs[i]\n",
    "        if (logL > maxLogL):\n",
    "            maxLogL = logL\n",
    "            maxIdx = i\n",
    "    if(maxIdx == twenty_test.target[n_art]):\n",
    "        predictionOk += 1\n",
    "print(\"Accuracy Test: \" + str(predictionOk/len(twenty_test.target)))"
   ]
  }
 ]
}